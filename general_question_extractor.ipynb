{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. helper funcs & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\server\\230621片题_数学\\Redo2\n"
     ]
    }
   ],
   "source": [
    "import threading,time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def multi(func,nums):\n",
    "    results = [None] * len(nums)\n",
    "\n",
    "    def compute_and_store_result(i, x):\n",
    "        results[i] = func(x)\n",
    "\n",
    "    threads = []\n",
    "    for i, x in enumerate(nums):\n",
    "        thread = threading.Thread(target=compute_and_store_result, args=(i, x))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in tqdm(threads, desc=\"Progress\"):\n",
    "        thread.join()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def get_all_files(folder_path):\n",
    "    file_paths = []\n",
    "    for root, directories, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "task_dir=os.getcwd()\n",
    "max_qnum=29\n",
    "\n",
    "idxs=[['{0}'.format(i) for i in range(1,max_qnum+1)],\n",
    "      ['(a)','(b)','(c)','(d)','(e)','(f)','(g)','(h)'],\n",
    "      ['(i)','(ii)','(iii)','(iv)','(v)','(vi)','(vii)']]\n",
    "\n",
    "print(task_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.get page images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect margin\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_numbers(numbers):\n",
    "    # Create x-axis values from 0 to (number of elements - 1)\n",
    "    x = range(len(numbers))\n",
    "    \n",
    "    # Plot the numbers\n",
    "    plt.plot(x, numbers)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Number')\n",
    "    plt.title('Plot of Numbers')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def convert_pixels_below_threshold(image, threshold):\n",
    "    # Convert image to numpy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Apply thresholding operation\n",
    "    image_array[image_array < threshold] = 255\n",
    "\n",
    "    # Convert numpy array back to image\n",
    "    result_image = Image.fromarray(image_array)\n",
    "\n",
    "    return result_image\n",
    "\n",
    "def detect_peaks(data,debug=False,max_mar=1/8):\n",
    "    peaks = []\n",
    "    n = len(data)\n",
    "    \n",
    "    # Check if the list has at least three elements\n",
    "    if n < 3:\n",
    "        return peaks\n",
    "    \n",
    "    # Check the first element\n",
    "    if data[0] >= data[1]:\n",
    "        is_peak = True\n",
    "    else:\n",
    "        is_peak = False\n",
    "    \n",
    "    start_index = 0\n",
    "    \n",
    "    # Iterate over the remaining elements\n",
    "    for i in range(1, n - 1):\n",
    "        # Check if the current element is greater than or equal to its neighbors\n",
    "        if data[i] >= data[i - 1] and data[i] >= data[i + 1]:\n",
    "            if not is_peak:\n",
    "                start_index = i\n",
    "                is_peak = True\n",
    "        else:\n",
    "            if is_peak:\n",
    "                end_index = i - 1\n",
    "                peak_value = max(data[start_index:end_index+1])\n",
    "                peak_width = end_index - start_index + 1\n",
    "                peaks.append((start_index, peak_width, peak_value))\n",
    "                is_peak = False\n",
    "    \n",
    "    # Check the last element\n",
    "    if is_peak:\n",
    "        end_index = n - 1\n",
    "        peak_value = max(data[start_index:end_index+1])\n",
    "        peak_width = end_index - start_index + 1\n",
    "        peaks.append((start_index, peak_width, peak_value))\n",
    "\n",
    "    if debug:\n",
    "        plot_numbers(data)\n",
    "    peaks = sorted(peaks,key=lambda x: -x[2])\n",
    "    ver_cut=[None,None]\n",
    "    for peak in peaks:\n",
    "        \n",
    "        position, width, value = peak\n",
    "        if debug:\n",
    "            print(f\"Peak detected at position {position}, width {width}, value {value}\")\n",
    "        if position<max_mar*len(data) and ver_cut[0] == None:\n",
    "            ver_cut[0]=position+width\n",
    "        \n",
    "        if position>(1-max_mar)*len(data) and ver_cut[1] == None:\n",
    "            ver_cut[1]=position-width\n",
    "\n",
    "        if None not in ver_cut:\n",
    "            break\n",
    "\n",
    "        \n",
    "    #print('cut:',ver_cut)\n",
    "\n",
    "    \n",
    "    return ver_cut\n",
    "\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import ImageDraw\n",
    "def get_margin(image,debug=False):\n",
    "\n",
    "    #display(image)\n",
    "    image_cover=convert_pixels_below_threshold(image,100)\n",
    "    #display(image_cover)\n",
    "    img_data=np.array(image_cover)\n",
    "    img_data_leftsum=255-img_data.mean(axis=0)\n",
    "    img_data_downsum=255-img_data.mean(axis=1)\n",
    "    \n",
    "    #plot_numbers(img_data_leftsum)\n",
    "    ver_cut=detect_peaks(img_data_leftsum,debug=debug)\n",
    "    \n",
    "\n",
    "    #plot_numbers(img_data_downsum)\n",
    "    hor_cut=detect_peaks(img_data_downsum,debug=debug)\n",
    "    if debug:\n",
    "        print('ver_cut:',ver_cut)\n",
    "        print('hor_cut:',hor_cut)\n",
    "    W,H=image.size\n",
    "    draw=ImageDraw.Draw(image)\n",
    "    # draw ver cut\n",
    "    for c in ver_cut:\n",
    "        draw.line((c,0,c,H),fill='red')\n",
    "\n",
    "    for c in hor_cut:\n",
    "        draw.line((0,c,W,c),fill='red')\n",
    "    if debug:\n",
    "        display(image)\n",
    "    left=ver_cut[0]\n",
    "    right=ver_cut[1]\n",
    "    top=hor_cut[0]\n",
    "    buttom=hor_cut[1]\n",
    "    return [left,top,right,buttom]\n",
    "    #for c in hor_cut:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    fp_dir=os.path.join(task_dir,'0-input')\n",
    "    #fp=random.choice(fps)\n",
    "    #pdf_fps=[fp]\n",
    "\n",
    "    pdf_fps=[fp for fp in get_all_files(fp_dir) if fp.endswith('.pdf')]\n",
    "    \n",
    "    for fp in pdf_fps:\n",
    "        pgs=convert_from_path(fp,grayscale=True,first_page=2,last_page=2)\n",
    "        print(fp)\n",
    "        get_margin(pgs[0],debug=True)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,PyPDF2\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "pdf_dir=os.path.join(task_dir,'0-input')\n",
    "pdf_fns=os.listdir(pdf_dir)\n",
    "pdf_fps=[os.path.join(pdf_dir,fn) for fn in pdf_fns if fn.endswith('.pdf')]\n",
    "pages_out_dir=os.path.join(task_dir,'1-pdf2images')\n",
    "\n",
    "\n",
    "def fp2imgs_crop(fp,remove_head=True,end_code='total for paper',debug=False):\n",
    "    \n",
    "    imgs=convert_from_path(fp,grayscale=True)\n",
    "    #print(len(imgs))\n",
    "    if end_code != '':\n",
    "        with open(fp, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in list(range(len(reader.pages)))[::-1]:\n",
    "                page = reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                #if len(text.replace('....',''))<50:\n",
    "                #    display(imgs[page_num])\n",
    "                if debug:\n",
    "                        print('page_num:',page_num)\n",
    "                        display(imgs[page_num])\n",
    "                        print(text)\n",
    "                if end_code in text.lower():\n",
    "                    imgs=imgs[:page_num+1]\n",
    "                    break\n",
    "\n",
    "    if remove_head:\n",
    "        imgs=imgs[1:]\n",
    "    precrops=get_margin(imgs[0])\n",
    "    for i,page in enumerate(imgs):\n",
    "        # precrop\n",
    "        imgs[i]=page.crop(tuple(precrops))\n",
    "\n",
    "    # save\n",
    "    fn=fp.split('\\\\')[-1].split('.')[0]\n",
    "    #print(fn)\n",
    "    cur_pdf_imgs_dir=os.path.join(pages_out_dir,fn)\n",
    "    #print(cur_pdf_imgs_dir)\n",
    "    if not os.path.exists(cur_pdf_imgs_dir):\n",
    "        os.makedirs(cur_pdf_imgs_dir)\n",
    "\n",
    "    pages = imgs\n",
    "\n",
    "    for ii,page in enumerate(pages):\n",
    "        image_fn=fn+'_'+\"page{:03d}\".format(ii+1)+'.png'\n",
    "        with open(os.path.join(cur_pdf_imgs_dir,image_fn), 'wb') as f:\n",
    "            page.save(f,format='PNG')\n",
    "            f.close()\n",
    "\n",
    "    return len(imgs)\n",
    "\n",
    "def test():\n",
    "    fp=random.choice(pdf_fps)\n",
    "    #fp=r'D:\\server\\230621片题_数学\\0-input\\wdm11_01_que_20230126.pdf'\n",
    "    print(fp)\n",
    "    num_imgs=fp2imgs_crop(fp,debug=True)\n",
    "    print('#imgs:',num_imgs)\n",
    "    #display(imgs[0])\n",
    "    #display(imgs[-1])\n",
    "\n",
    "#test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\n",
      "#paper: 1 #page: 26 avg page per paper: 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "page_lens=multi(fp2imgs_crop,pdf_fps)\n",
    "print(page_lens)\n",
    "print('#paper:',len(page_lens),'#page:',sum(page_lens),'avg page per paper:',sum(page_lens)/len(page_lens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.delete appendix pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\server\\\\230621片题_数学\\\\Redo2\\\\2-pure_questions_images\\\\question_images'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil,os\n",
    "pages_out_dir=os.path.join(task_dir,'1-pdf2images')\n",
    "# destination folder path\n",
    "pure_img_folder = os.path.join(task_dir,'2-pure_questions_images','question_images')\n",
    "deleted_dir=os.path.join(task_dir,'2-pure_questions_images','deleted_images')\n",
    "if not os.path.exists(deleted_dir):\n",
    "    os.makedirs(deleted_dir)\n",
    "# copy the folder and its contents\n",
    "shutil.copytree(pages_out_dir, pure_img_folder,dirs_exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import Output\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "pytesseract.pytesseract.tesseract_cmd = r'E:\\Tesseract\\tesseract.exe'\n",
    "from PIL import Image,ImageFont,ImageDraw, ImageFilter\n",
    "\n",
    "def delete_no_text(fp,debug=False,min_cnum=54):\n",
    "    im=Image.open(fp)\n",
    "    txt=pytesseract.image_to_string(im)\n",
    "    if debug:\n",
    "            print(fp.split('\\\\')[-1], len(txt),txt)\n",
    "            display(im)\n",
    "    if len(txt)<min_cnum:\n",
    "        fn=fp.split('\\\\')[-1]\n",
    "        if not debug:\n",
    "            shutil.move(dst=os.path.join(deleted_dir,fn),src=fp)\n",
    "        \n",
    "        return True\n",
    "        #print('deleted '+fn)\n",
    "    else:\n",
    "         return False\n",
    "    \n",
    "# test\n",
    "def test():\n",
    "    deleted=[]\n",
    "    fps=get_all_files(r'D:\\server\\230621数学临时片题\\1-pdf2images\\P1 2022 1月 问卷')\n",
    "    for fp in fps:\n",
    "        if delete_no_text(fp,debug=True):\n",
    "            deleted.append(fp)\n",
    "    print(len(deleted))\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 26/26 [00:01<00:00, 25.92it/s]\n"
     ]
    }
   ],
   "source": [
    "fps=get_all_files(pure_img_folder)\n",
    "dels=multi(delete_no_text,fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted 9 from 26, left 17\n"
     ]
    }
   ],
   "source": [
    "print('deleted {0} from {1}, left {2}'.format(len([d for d in dels if d]),len(dels),len([d for d in dels if not d])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now, open the folder and check the images, delete cover and appendix pages that does not contain questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.OCR analyze cut to strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import Output\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "pytesseract.pytesseract.tesseract_cmd = r'E:\\Tesseract\\tesseract.exe'\n",
    "from PIL import Image,ImageFont,ImageDraw, ImageFilter\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\server\\230620片题_物理\\2-pure_questions_images\\question_images\\6ph04_01_que_20170616\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\server\\\\230620片题_物理\\\\2-pure_questions_images\\\\question_images\\\\6ph04_01_que_20170616'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 132\u001b[0m\n\u001b[0;32m    129\u001b[0m         crop_imgs\u001b[39m=\u001b[39mcutter(pg,debug\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(crop_imgs))\n\u001b[1;32m--> 132\u001b[0m test()\n",
      "Cell \u001b[1;32mIn[30], line 128\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m fps:\n\u001b[0;32m    127\u001b[0m     \u001b[39mprint\u001b[39m(p)\n\u001b[1;32m--> 128\u001b[0m     pg\u001b[39m=\u001b[39mImage\u001b[39m.\u001b[39;49mopen(p)\n\u001b[0;32m    129\u001b[0m     crop_imgs\u001b[39m=\u001b[39mcutter(pg,debug\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(crop_imgs))\n",
      "File \u001b[1;32mc:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3130\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3131\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3132\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\server\\\\230620片题_物理\\\\2-pure_questions_images\\\\question_images\\\\6ph04_01_que_20170616'"
     ]
    }
   ],
   "source": [
    "from PIL import ImageDraw\n",
    "def im2rects(img): \n",
    "    '''Page segmentation modes:\n",
    "  0    Orientation and script detection (OSD) only.\n",
    "  1    Automatic page segmentation with OSD.\n",
    "  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "  4    Assume a single column of text of variable sizes.\n",
    "  5    Assume a single uniform block of vertically aligned text.\n",
    "  6    Assume a single uniform block of text.\n",
    "  7    Treat the image as a single text line.\n",
    "  8    Treat the image as a single word.\n",
    "  9    Treat the image as a single word in a circle.\n",
    " 10    Treat the image as a single character.\n",
    " 11    Sparse text. Find as much text as possible in no particular order.\n",
    " 12    Sparse text with OSD.\n",
    " 13    Raw line. Treat the image as a single text line,\n",
    "       bypassing hacks that are Tesseract-specific.'''\n",
    "    custom_config = r'--psm 3'\n",
    "    d=pytesseract.image_to_data(img,output_type=Output.DICT,config=custom_config)\n",
    "    n_boxes = len(d['text'])\n",
    "\n",
    "    W,H = img.size\n",
    "    rectangles=[]\n",
    "    min_y=H\n",
    "    min_yi=0\n",
    "    for i in range(n_boxes):\n",
    "        if int(d['level'][i]) == 4:\n",
    "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "            rectangles.append([(x, y), (x + w, y + h)])\n",
    "            if y < min_y:\n",
    "                min_y=y\n",
    "                min_yi=len(rectangles)-1\n",
    "            #draw.text((x, y), d['text'][i], align =\"left\",fill=\"red\")\n",
    "    #rectangles.pop(min_yi)\n",
    "    return rectangles\n",
    "    \n",
    "    \n",
    "def cutter(img,debug=False,precrops=[0,0,0,0],min_dy_rate=0.015,x_tol_rate=0.22):\n",
    "    '''\n",
    "     input: Image\n",
    "     precrops: list of 4 floats between 0 and 1, left top right bottom margin rate\n",
    "     output: list of cropped images\n",
    "    '''\n",
    "    W,H = img.size\n",
    "\n",
    "    \n",
    "    pre_left=int(W*precrops[0])\n",
    "    pre_top=int(H*precrops[1])\n",
    "    pre_right=int(W*(1-precrops[2]))\n",
    "    pre_bottom=int(H*(1-precrops[3]))\n",
    "    # precrop\n",
    "    img=img.crop(tuple([pre_left,pre_top,pre_right,pre_bottom]))\n",
    "    #display(img)\n",
    "    W,H = img.size\n",
    "    \n",
    "    draw=ImageDraw.Draw(img)\n",
    "    rectangles=im2rects(img)\n",
    "    rectangles=sorted(rectangles,key=lambda x:x[0][0])\n",
    "    if len(rectangles)==0:\n",
    "        return []\n",
    "    pad=5\n",
    "    \n",
    "    ymax=max([i[1][1] for i in rectangles])+pad\n",
    "    ymin=min([i[0][1] for i in rectangles])-pad\n",
    "    #xmax=max([i[1][0] for i in rectangles])+pad\n",
    "    #xmin=min([i[0][0] for i in rectangles])-pad\n",
    "    xmax=W\n",
    "    xmin=0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    x_star=rectangles[0][0][0]\n",
    "    x_tolerance=x_tol_rate*W\n",
    "    cut_ys=[ymax,ymin,H,0]\n",
    "    \n",
    "    for rect in rectangles:\n",
    "        if debug:\n",
    "            draw.rectangle(rect,outline='red')\n",
    "        if abs(rect[0][0]) < x_tolerance:\n",
    "            cut_ys.append(rect[0][1]-pad)\n",
    "    cut_ys=sorted(list(set(cut_ys)))\n",
    "    # set min dy rate to prevent very small dy\n",
    "    old_y=cut_ys[0]\n",
    "    stable_cut_ys=[old_y]\n",
    "    dy_tol=min_dy_rate*H\n",
    "    for new_y in cut_ys[1:]:\n",
    "        dy = new_y - old_y\n",
    "        if dy > dy_tol or new_y==H:\n",
    "            stable_cut_ys.append(new_y)\n",
    "            old_y=new_y\n",
    "\n",
    "    cut_ys=stable_cut_ys\n",
    "        \n",
    "    crop_imgs=[]\n",
    "    for i,y in enumerate(cut_ys[:-1]):\n",
    "        crop_imgs.append(img.crop((xmin,y,xmax,cut_ys[i+1])))\n",
    "        \n",
    "    if debug:\n",
    "        print('rectangles:',rectangles)\n",
    "        print('ymin:',ymin,'ymax',ymax)\n",
    "        print('cut into',len(cut_ys)-1,', y positions:',cut_ys)\n",
    "        draw.line([(0,dy_tol),(W,dy_tol)],fill =\"green\", width = 0)\n",
    "        draw.line([(int(x_tolerance),0),(int(x_tolerance),H)],fill =\"green\", width = 0)\n",
    "        for y in cut_ys:\n",
    "            draw.line([(0,y),(W,y)],fill ='blue', width = 0)\n",
    "\n",
    "        draw.line([(xmin,0),(xmin,H)],fill =\"blue\", width = 0)\n",
    "        draw.line([(xmax,0),(xmax,H)],fill =\"blue\", width = 0)\n",
    "        print('raw img:')\n",
    "        display(img)\n",
    "        print('cropped imgs:')\n",
    "        for im in crop_imgs:\n",
    "            print('-----')\n",
    "            display(im)\n",
    "        \n",
    "    return crop_imgs\n",
    "\n",
    "\n",
    "def test():\n",
    "    pure_dir=os.path.join(pure_img_folder)\n",
    "    fps=[p for p in get_all_files(pure_dir) if p.endswith('.png')]\n",
    "    fps=random.sample(fps,5)\n",
    "    fps=[r'D:\\server\\230620片题_物理\\2-pure_questions_images\\question_images\\6ph04_01_que_20170616']\n",
    "    for p in fps:\n",
    "        print(p)\n",
    "        pg=Image.open(p)\n",
    "        crop_imgs=cutter(pg,debug=True)\n",
    "        print(len(crop_imgs))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 6/6 [00:00<00:00,  9.48it/s]\n",
      "                                             \r"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "images_dir=pure_img_folder\n",
    "strips_dir=os.path.join(task_dir,'3-images2stripes')\n",
    "paper_names=os.listdir(images_dir)\n",
    "paper_images_dirs=[os.path.join(images_dir,p) for p in paper_names]\n",
    "#display(paper_images_dirs)\n",
    "\n",
    "for i,paper_images_dir in tqdm(enumerate(paper_images_dirs),total=len(paper_images_dirs),leave=False):\n",
    "    paper_name=paper_names[i]\n",
    "    paper_strip_dir=os.path.join(strips_dir,paper_name)\n",
    "    if not os.path.exists(paper_strip_dir):\n",
    "        os.makedirs(paper_strip_dir)\n",
    "    img_fns=sorted([fn for fn in os.listdir(paper_images_dir) if fn.endswith('.png')])\n",
    "    imgs=[Image.open(os.path.join(paper_images_dir,img_fn)) for img_fn in img_fns]\n",
    "    \n",
    "    strips=[]\n",
    "    strip_fns=[]\n",
    "    cur_stripss=multi(cutter,imgs)\n",
    "    for i,(img_fn,img) in enumerate(zip(img_fns,imgs)):\n",
    "        #display(img)\n",
    "        cur_strips=cur_stripss[i]\n",
    "        #print(img_fn+' cut into',len(cur_strips))\n",
    "        cur_strip_fns=[img_fn.replace('.png','_strip{:04d}.png'.format(idx+1)) for idx in range(len(cur_strips))]\n",
    "        strips+=cur_strips\n",
    "        strip_fns+=cur_strip_fns\n",
    "\n",
    "    for j,s in enumerate(strips):\n",
    "        with open(os.path.join(paper_strip_dir,strip_fns[j]),'wb') as f:\n",
    "            s.save(f,format='PNG')\n",
    "            f.close()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.extract eigentext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading,os\n",
    "from PIL import Image\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    for root, directories, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "def fp2txt_half(fp,half_rate=0.22,config=r'--psm 7'):\n",
    "    img=Image.open(fp)\n",
    "    W,H=img.size\n",
    "    img=img.crop((0,0,int(W*half_rate),H))\n",
    "    return pytesseract.image_to_string(img,config=config)\n",
    "\n",
    "def fp2txt_full(fp):\n",
    "    return fp2txt_half(fp,half_rate=1,config=r'--psm 3')\n",
    "\n",
    "def test():\n",
    "    src_dir=os.path.join(task_dir,'3-images2stripes')\n",
    "    fps=[p for p in get_all_files(src_dir) if p.endswith('.png')]\n",
    "    fps=random.sample(fps,100)\n",
    "    #fps=[r'D:\\server\\230615test_片题2\\3-images2stripes\\2201ALPhysicsU1\\2201ALPhysicsU1_page017_strip0009.png']\n",
    "    for fp in fps:\n",
    "        print(fp)\n",
    "        display(Image.open(fp))\n",
    "        print('text_half:',fp2txt_half(fp))\n",
    "        print('text_full:',fp2txt_full(fp),'\\n\\n')\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 115 strips from d:\\server\\230621片题_数学\\Redo2\\3-images2stripes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 115/115 [00:03<00:00, 34.41it/s]\n",
      "Progress: 100%|██████████| 115/115 [00:02<00:00, 39.09it/s]\n"
     ]
    }
   ],
   "source": [
    "strips_dir=os.path.join(task_dir,'3-images2stripes')\n",
    "all_strips_fps=get_file_paths(strips_dir)\n",
    "\n",
    "# sanity check\n",
    "for p in all_strips_fps:\n",
    "    assert p.endswith('.png')\n",
    "\n",
    "strips_excel_fp=os.path.join(task_dir,'4-strips_info.xlsx')\n",
    "print('load {0} strips from {1}'.format(len(all_strips_fps),strips_dir))\n",
    "\n",
    "st_txts_head=multi(fp2txt_half,all_strips_fps)\n",
    "st_txts_full=multi(fp2txt_full,all_strips_fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st_fps=[]\n",
    "st_pns=[]\n",
    "st_pgns=[]\n",
    "st_stns=[]\n",
    "\n",
    "for i,fp in enumerate(all_strips_fps):\n",
    "\n",
    "    fn=fp.split('\\\\')[-1]\n",
    "    st_fps.append(fp)\n",
    "    st_pns.append(fn.split('_page')[0])\n",
    "    st_pgns.append(fn.split('_page')[-1].split('_strip')[0])\n",
    "    st_stns.append(fn.split('_strip')[-1].split('.png')[0])\n",
    "    img=Image.open(fp)\n",
    "    #display(img)\n",
    "    #st_txts.append(pytesseract.image_to_string(img,config='r--psm 7'))\n",
    "    #print(text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "data=pd.DataFrame({'file_path':st_fps,\n",
    "                   'paper_name':st_pns,\n",
    "                   'page_num':st_pgns,\n",
    "                   'strip_num':st_stns,\n",
    "                   'text_head':st_txts_head,\n",
    "                   'text_full':st_txts_full\n",
    "                   })\n",
    "\n",
    "data.to_excel(strips_excel_fp,index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-recongnize q_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "d:\\server\\230621片题_数学\\Redo2\\5-recognize_qnum.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def qnums_valid(qnums,idxs=idxs):\n",
    "    \n",
    "    def is_continue(qn1,qn2):\n",
    "        #print(qn1,qn2)\n",
    "        lvl=len(qn1)\n",
    "        if qn1==qn2:\n",
    "            return True\n",
    "        \n",
    "        if qn1[0]=='':\n",
    "            return qn2[0]==idxs[3-lvl][0]\n",
    "\n",
    "        if qn1[0]==qn2[0]:\n",
    "            if len(qn1)!=1 and len(qn2)!=1:\n",
    "                return is_continue(qn1[1:],qn2[1:])\n",
    "            else:\n",
    "                return True\n",
    "        return qn2[0] == nxt_qidx(qn1[0])\n",
    "    \n",
    "    def nxt_qidx(idx):\n",
    "        for cur_lvl_idxs in idxs:\n",
    "            if idx in cur_lvl_idxs:\n",
    "                posi=cur_lvl_idxs.index(idx)\n",
    "                if posi==len(cur_lvl_idxs)-1:\n",
    "                    return idx\n",
    "                else:\n",
    "                    return cur_lvl_idxs[posi+1]\n",
    "        return None\n",
    "    \n",
    "\n",
    "    r=[]\n",
    "    qnums=[q.split('-') for q in qnums]\n",
    "    qnums=[q+(3-len(q))*[''] for q in qnums]\n",
    "\n",
    "    qn1=['']*3\n",
    "    for qn2 in qnums:\n",
    "        \n",
    "        is_conti=is_continue(qn1,qn2)\n",
    "        #print(qn1,qn2,is_conti)\n",
    "        if not is_conti and len(r)>0:\n",
    "            r[-1]=False\n",
    "        r.append(is_conti)\n",
    "        \n",
    "        qn1=qn2\n",
    "    return r\n",
    "\n",
    "input_excel_fp=os.path.join(task_dir,'4-strips_info.xlsx')\n",
    "out_excel_fp=os.path.join(task_dir,'5-recognize_qnum.xlsx')\n",
    "\n",
    "data=pd.read_excel(input_excel_fp)\n",
    "\n",
    "# get pre_q_num1\n",
    "txts_head=list(map(str,data['text_head']))\n",
    "txts_full=list(map(str,data['text_full']))\n",
    "txts=[]\n",
    "for head,full in zip(txts_head,txts_full):\n",
    "    if head[0].isalpha():\n",
    "        txts.append(full[:10])\n",
    "    elif full[0].isalpha():\n",
    "        txts.append(head[:10])\n",
    "    else:\n",
    "        txts.append(head[:7]+' '+full[:5])\n",
    "\n",
    "pre_q_nums1=[]\n",
    "print(len(txts))\n",
    "lvls_idxss=[idxs[0][::-1],idxs[1],idxs[2]]\n",
    "for txt in txts:\n",
    "    r=[]\n",
    "    txt_idxs=[]\n",
    "    for lvl_idxs in lvls_idxss:\n",
    "        for idx in lvl_idxs:\n",
    "            if idx in txt:\n",
    "                r.append(idx)\n",
    "                break\n",
    "    r='-'.join(r)\n",
    "    pre_q_nums1.append(r)\n",
    "\n",
    "data['pre_q_num1']=pre_q_nums1\n",
    "data.to_excel(out_excel_fp,index=False)\n",
    "print(out_excel_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_q_num1_finish(in_fp,out_excel_fp):\n",
    "    # reguliarize\n",
    "    data=pd.read_excel(in_fp)\n",
    "    pns=list(sorted(set(data['paper_name'])))\n",
    "    datas=[data[data['paper_name']==pn] for pn in pns]\n",
    "    aggregate_by_full_idx_datas=[]\n",
    "    for pn,datai in zip(pns,datas):\n",
    "        #datai=data[data['paper_name']==pn]\n",
    "        print(pn)\n",
    "        datai['pre_q_num1']=datai['pre_q_num1'].astype(str).replace('nan','')\n",
    "        datai['text_full']=datai['text_full'].astype(str).replace('nan','')\n",
    "        #datai['text_head']=datai['text_head'].astype(str).replace('nan','')\n",
    "        pn_qnums=datai['pre_q_num1']\n",
    "        pn_ftxts=datai['text_full']\n",
    "        #pn_htxts=datai['text_half']\n",
    "        last_valid_qn=''\n",
    "        expected_cur_qn=''\n",
    "        valid_qns=[]\n",
    "        for cur_qn in pn_qnums:\n",
    "            if '-' in cur_qn:\n",
    "                cur_qn=cur_qn.split('-')[0]\n",
    "\n",
    "            if cur_qn in idxs[0]:\n",
    "                #print(cur_qn)\n",
    "                if last_valid_qn =='':\n",
    "                    if cur_qn==idxs[0][0]:\n",
    "                        last_valid_qn=cur_qn\n",
    "                        expected_cur_qn=idxs[0][idxs[0].index(last_valid_qn)+1]\n",
    "\n",
    "                        #print('c',cur_qn)\n",
    "                \n",
    "                elif cur_qn != last_valid_qn and cur_qn==expected_cur_qn:\n",
    "                    #print(last_valid_qn)\n",
    "                    last_valid_qn=cur_qn\n",
    "                    expected_cur_qn=idxs[0][idxs[0].index(last_valid_qn)+1]\n",
    "\n",
    "            valid_qns.append(last_valid_qn)\n",
    "                #print(cur_qn,expected_cur_qn,last_valid_qn)\n",
    "        assert len(pn_qnums)==len(valid_qns)\n",
    "        datai['valid_q_num']=valid_qns\n",
    "\n",
    "        # append next level idxs\n",
    "        full_idxs=[]\n",
    "        #full_txts=[]\n",
    "        cur_stack=['','','']\n",
    "        #cur_txt=['','','']\n",
    "        for valid_q_num, pn_qnum_whole, pn_ftxt in zip(valid_qns,pn_qnums,pn_ftxts):\n",
    "            pn_qnum_lvls=pn_qnum_whole.split('-')\n",
    "            for pn_qnum in pn_qnum_lvls:\n",
    "                # new lvl 1 question\n",
    "                if valid_q_num!=cur_stack[0]:\n",
    "                    cur_stack=[valid_q_num,'','']\n",
    "                else:\n",
    "                    # new lvl 2 question\n",
    "                    if pn_qnum in idxs[1]:\n",
    "                        if pn_qnum!=cur_stack[1]:\n",
    "                            cur_stack=[cur_stack[0],pn_qnum,'']\n",
    "                    # new lvl 3 question\n",
    "                    elif pn_qnum in idxs[2]:\n",
    "                        if pn_qnum!=cur_stack[2]:\n",
    "                            cur_stack=[cur_stack[0],cur_stack[1],pn_qnum]\n",
    "\n",
    "                \n",
    "\n",
    "            #print('valid_qnum:',valid_q_num, 'pn_qnum:',pn_qnum,'cur_stack:',cur_stack)\n",
    "            full_idxs.append('-'.join([c for c in cur_stack if c !='']))\n",
    "            #print(cur_txt)\n",
    "            #print('\\n'.join([str(c).replace('\\n',' ') for c in cur_txt if c !='']))\n",
    "            #full_txts.append('\\n'.join([str(c).replace('\\n',' ') for c in cur_txt if c !='']))\n",
    "        datai['q_num_full']=full_idxs\n",
    "        datai['valid']=qnums_valid(full_idxs)\n",
    "        \n",
    "\n",
    "        aggregate_by_full_idx_data=pd.DataFrame()\n",
    "        agg_strip_fpss=[]\n",
    "        agg_paper_names=[]\n",
    "        agg_page_nums=[]\n",
    "        agg_txt_full=[]\n",
    "        agg_qnums=[]\n",
    "        agg_full_idx=[]\n",
    "        agg_valids=[]\n",
    "        \n",
    "\n",
    "        # aggregate by unique full_idx \n",
    "        change_ats=[0]\n",
    "        qs_full=[datai['q_num_full'].iloc[0]]\n",
    "        for i in range(1,len(datai)):\n",
    "            if datai['q_num_full'].iloc[i]!= datai['q_num_full'].iloc[i-1]:\n",
    "                change_ats.append(i)\n",
    "                qs_full.append(datai['q_num_full'].iloc[i])\n",
    "        change_ats.append(len(datai))\n",
    "\n",
    "                \n",
    "    \n",
    "        for i in range(len(qs_full)):\n",
    "            cur_full_idx=qs_full[i]\n",
    "            cur_full_idx_data=datai.iloc[change_ats[i]:change_ats[i+1]]\n",
    "            #display(cur_full_idx_data)\n",
    "            if cur_full_idx =='':\n",
    "                continue\n",
    "                \n",
    "            agg_strip_fpss.append('\\n'.join([fp for fp in cur_full_idx_data['file_path']]))\n",
    "            agg_paper_names.append(list(cur_full_idx_data['paper_name'])[0])\n",
    "            agg_page_nums.append(list(cur_full_idx_data['page_num'])[0])\n",
    "            agg_txt_full.append(' '.join([t for t in cur_full_idx_data['text_full']]).replace('\\n',''))\n",
    "            agg_qnums.append(str(list(cur_full_idx_data['valid_q_num'])[0]))\n",
    "            agg_full_idx.append(cur_full_idx)\n",
    "            agg_valids.append(all(list(cur_full_idx_data['valid'])))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        # for cur_full_idx in list(dict.fromkeys(datai['q_num_full'])):\n",
    "        #     if cur_full_idx =='':\n",
    "        #         continue\n",
    "        #     #print(cur_full_idx)\n",
    "        #     cur_full_idx_data=datai[datai['q_num_full']==cur_full_idx]\n",
    "\n",
    "        #     agg_strip_fpss.append('\\n'.join([fp for fp in cur_full_idx_data['file_path']]))\n",
    "        #     agg_paper_names.append(list(cur_full_idx_data['paper_name'])[0])\n",
    "        #     agg_page_nums.append(list(cur_full_idx_data['page_num'])[0])\n",
    "        #     agg_txt_full.append(' '.join([t for t in cur_full_idx_data['text_full']]).replace('\\n',''))\n",
    "        #     agg_qnums.append(str(list(cur_full_idx_data['valid_q_num'])[0]))\n",
    "        #     agg_full_idx.append(cur_full_idx)\n",
    "\n",
    "\n",
    "        aggregate_by_full_idx_data['agg_strip_fpss']=agg_strip_fpss\n",
    "        aggregate_by_full_idx_data['agg_paper_names']=agg_paper_names\n",
    "        aggregate_by_full_idx_data['agg_page_nums']=agg_page_nums\n",
    "        aggregate_by_full_idx_data['agg_txt_full']=agg_txt_full\n",
    "        aggregate_by_full_idx_data['agg_qnum']=agg_qnums\n",
    "        aggregate_by_full_idx_data['agg_full_qnum']=agg_full_idx\n",
    "        aggregate_by_full_idx_data['agg_valids']=agg_valids\n",
    "\n",
    "        aggregate_by_full_idx_datas.append(aggregate_by_full_idx_data)\n",
    "\n",
    "    with pd.ExcelWriter(out_excel_fp) as writer:\n",
    "        pd.concat(datas).to_excel(writer,sheet_name='raw',index=False)\n",
    "        pd.concat(aggregate_by_full_idx_datas).to_excel(writer,sheet_name='agg_by_full_qnum',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdm11_01_que_20230126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_q_num1_finish(in_fp=out_excel_fp,out_excel_fp=out_excel_fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\server\\\\230621片题_数学\\\\Redo2\\\\6-sanity_checkpoint\\\\sanity_check.xlsx'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "step_dir_6=os.path.join(task_dir,'6-sanity_checkpoint')\n",
    "if not os.path.exists(step_dir_6):\n",
    "    os.mkdir(step_dir_6)\n",
    "src=os.path.join(task_dir,'5-recognize_qnum.xlsx')\n",
    "dst=os.path.join(task_dir,step_dir_6,'sanity_check.xlsx')\n",
    "shutil.copy(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def concatenate_images_vertically(file_paths):\n",
    "    images = [Image.open(file_path) for file_path in file_paths]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "    total_width = max(widths)\n",
    "    total_height = sum(heights)\n",
    "\n",
    "    concatenated_image = Image.new('RGB', (total_width, total_height))\n",
    "\n",
    "    y_offset = 0\n",
    "    for image in images:\n",
    "        concatenated_image.paste(image, (0, y_offset))\n",
    "        y_offset += image.height\n",
    "\n",
    "    return concatenated_image\n",
    "\n",
    "\n",
    "def fps2imgs(fps):\n",
    "    #if len(fps)>2 and fps[-1].split('page')[-1].split('_')[0]!=fps[-2].split('page')[-2].split('_')[0]:\n",
    "    #    fps=fps[:-1]\n",
    "\n",
    "    fpss=[]\n",
    "    old_idx=0\n",
    "    old_pgn=fps[0].split('page')[-1].split('_')[0]\n",
    "    for i,p in enumerate(fps):\n",
    "        new_pgn=p.split('page')[-1].split('_')[0]\n",
    "        #print(old_pgn,new_pgn)\n",
    "        if new_pgn != old_pgn:\n",
    "            fpss.append(fps[old_idx:i])\n",
    "            old_idx=i\n",
    "            old_pgn=new_pgn\n",
    "    if old_idx<len(fps)-1:\n",
    "        fpss.append(fps[old_idx:])\n",
    "    \n",
    "    imgs=multi(concatenate_images_vertically,fpss)\n",
    "    #for fs in fpss:\n",
    "    #    imgs.append(concatenate_images_vertically(fs))\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "def pn2q_imgs(pn,out_dir=os.path.join(task_dir,'6-sanity_checkpoint')):\n",
    "    #print(pn)\n",
    "    pn_out_dir=os.path.join(out_dir,pn)\n",
    "    if not os.path.exists(pn_out_dir):\n",
    "        os.makedirs(pn_out_dir)\n",
    "    pn_data=data[data['agg_paper_names']==pn]\n",
    "\n",
    "\n",
    "    for k in ['agg_strip_fpss','agg_txt_full','agg_valids']:\n",
    "        #print(k)\n",
    "        #display(pn_data)\n",
    "        if k == 'agg_valids':\n",
    "            pn_data[k] = pn_data[[k,'agg_qnum']].groupby(['agg_qnum'])[k].transform(lambda x: all([xx=='True' for xx in x]))\n",
    "            continue\n",
    "        pn_data[k] = pn_data[[k,'agg_qnum']].groupby(['agg_qnum'])[k].transform(lambda x: '\\n'.join(x))\n",
    "        \n",
    "\n",
    "    #display(pn_data)\n",
    "    df2=pn_data[['agg_strip_fpss','agg_txt_full','agg_qnum','agg_paper_names','agg_valids']].drop_duplicates()\n",
    "    #display(df2)\n",
    "    #df2.to_excel(os.path.join(out_dir,'mm.xlsx'),index=False)\n",
    "\n",
    "    q_imgs_fpss=[]\n",
    "    for i in range(len(df2)):\n",
    "        q_num=df2['agg_qnum'].iloc[i]\n",
    "        fps=df2['agg_strip_fpss'].iloc[i].split('\\n')\n",
    "        #for p in fps:\n",
    "            #print(p)\n",
    "\n",
    "        imgs=fps2imgs(fps)\n",
    "        q_imgs_fps=[]\n",
    "        for pic_i,im in enumerate(imgs):\n",
    "            fn='Q{:03d}-pic{:02d}.png'.format(i+1,pic_i+1)\n",
    "            fp=os.path.join(pn_out_dir,fn)\n",
    "            im.save(fp)\n",
    "            q_imgs_fps.append(fp)\n",
    "        q_imgs_fpss.append('\\n'.join(q_imgs_fps))\n",
    "    df2['q_imgs_fpss']=q_imgs_fpss\n",
    "\n",
    "    df2.to_excel(os.path.join(pn_out_dir,pn+'.xlsx'),index=False)\n",
    "\n",
    "    return all(list(df2['agg_valids']))\n",
    "\n",
    "def test():\n",
    "    input_excel_fp=os.path.join(task_dir,'5-recognize_qnum.xlsx')\n",
    "    data=pd.read_excel(input_excel_fp,sheet_name='agg_by_full_qnum',dtype=str).astype(str)\n",
    "    data['agg_full_qnum']=data['agg_full_qnum'].astype(str)\n",
    "    pns=list(dict.fromkeys(data['agg_paper_names']))\n",
    "    multi(pn2q_imgs,pns[:2])\n",
    "\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdm11_01_que_20230126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 90.91it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "in_fp=os.path.join(step_dir_6,'sanity_check.xlsx')\n",
    "pre_q_num1_finish(in_fp=in_fp,out_excel_fp=in_fp)\n",
    "\n",
    "\n",
    "data=pd.read_excel(in_fp,sheet_name='agg_by_full_qnum',dtype=str).astype(str)\n",
    "data['agg_full_qnum']=data['agg_full_qnum'].astype(str)\n",
    "pns=list(dict.fromkeys(data['agg_paper_names']))\n",
    "multi(pn2q_imgs,pns)\n",
    "\n",
    "# copy strips\n",
    "for pn in pns:\n",
    "    src=os.path.join(task_dir,'3-images2stripes',pn)\n",
    "    dst=os.path.join(step_dir_6,pn,'strips')\n",
    "    shutil.copytree(src,dst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, open the '6-sanity_check.xlsx' and manually check&fix the pre_qnum1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d1 2020 1月 问卷']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m             rd\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39massert\u001b[39;00m rd\u001b[39m==\u001b[39m\u001b[39mlen\u001b[39m(bad_pns)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "step_dir_6=os.path.join(task_dir,'6-sanity_checkpoint')\n",
    "def delete_rows_by_names(names, file_path, column_name, out_path=None):\n",
    "    if out_path == None:\n",
    "        out_path=file_path\n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Filter out the rows where the given column's value is in the list of names\n",
    "    df = df[~df[column_name].isin(names)]\n",
    "    \n",
    "    # Save the modified DataFrame back to the Excel file\n",
    "    df.to_excel(out_path, index=False)\n",
    "\n",
    "bad_pns='''d1 2020 1月 问卷'''.split('\\n')\n",
    "display(bad_pns)\n",
    "\n",
    "white_fp=os.path.join(step_dir_6,'sanity_check_white.xlsx')\n",
    "delete_rows_by_names(bad_pns,\n",
    "                     os.path.join(step_dir_6,'sanity_check.xlsx'),\n",
    "                     'paper_name',\n",
    "                     out_path=white_fp)\n",
    "\n",
    "\n",
    "# move bad paper into Redo\n",
    "import shutil\n",
    "os.makedirs(os.path.join(task_dir,'Redo','0-input'))\n",
    "rd=0\n",
    "for fp in get_all_files(os.path.join(task_dir,'0-input')):\n",
    "    #print(fp)\n",
    "    for pn in bad_pns:\n",
    "        if pn+'.pdf' == fp.split('\\\\')[-1]:\n",
    "            shutil.copy(fp,os.path.join(task_dir,'Redo','0-input',fp.split('\\\\')[-1]))\n",
    "            rd+=1\n",
    "            break\n",
    "assert rd==len(bad_pns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-concate image & output as task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_dir_6=os.path.join(task_dir,'6-sanity_checkpoint')\n",
    "step_dir_7=os.path.join(task_dir,'7-concat_image')\n",
    "def pn2q_imgs2(pn,out_dir=step_dir_7):\n",
    "    return pn2q_imgs(pn,out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdm11_01_que_20230126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 125.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Progress: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(step_dir_7):\n",
    "    os.mkdir(step_dir_7)\n",
    "\n",
    "in_fp=os.path.join(step_dir_6,'sanity_check.xlsx')\n",
    "out_fp=os.path.join(step_dir_7,'concate_image.xlsx')\n",
    "pre_q_num1_finish(in_fp=in_fp,out_excel_fp=out_fp)\n",
    "\n",
    "data=pd.read_excel(out_fp,sheet_name='agg_by_full_qnum',dtype=str).astype(str)\n",
    "data['agg_full_qnum']=data['agg_full_qnum'].astype(str)\n",
    "pns=list(dict.fromkeys(data['agg_paper_names']))\n",
    "multi(pn2q_imgs2,pns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to task1\n",
    "def fp2fp(in_fp,out_fp):\n",
    "    data=pd.read_excel(in_fp)\n",
    "    data_frq=data[data['Ques_type']=='FRQ']\n",
    "    data_mcq=data[data['Ques_type']=='MCQ']\n",
    "\n",
    "\n",
    "    txts,q_nums,p_names=list(data_frq['Ques_text']),list(data_frq['Ques_number']),list(data_frq['Paper_name'])\n",
    "\n",
    "    q_txts=txts\n",
    "    out_frq_data=pd.DataFrame(columns=list(data.keys()))\n",
    "    q_num2txt=dict()\n",
    "\n",
    "    q_n=[]\n",
    "    q_tx=[]\n",
    "    q_pn=[]\n",
    "    q_type=[]\n",
    "\n",
    "    for i,t in enumerate(q_txts):\n",
    "        q_num=str(q_nums[i])\n",
    "        #print(t)\n",
    "        t=str(t)\n",
    "        if len(t)==0 or t=='nan':\n",
    "            q_n.append(q_num)\n",
    "            q_tx.append('')\n",
    "            q_pn.append(p_names[i])\n",
    "            q_type.append('FRQ')\n",
    "        qs=break_q(t)\n",
    "        if qs['context']!='can not break':\n",
    "            #print(q_num+'-context: '+qs['context'])\n",
    "            for k in qs:\n",
    "                if k != 'original' and k != 'context':\n",
    "                    qs[k]=break_q(qs[k],idx_pointers=['(i)','(ii)','(iii)','(iv)','(v)'])\n",
    "                    if qs[k]['context']!='can not break':\n",
    "                        #print(q_num+'-'+k+'-context: '+qs[k]['context'])\n",
    "                        for kk in qs[k]:\n",
    "                            if kk != 'original' and kk!='context':\n",
    "                                q_idx=q_num+'-'+k+'-'+kk\n",
    "                                t=qs['context']+qs[k]['context']+qs[k][kk]\n",
    "                                #print(q_idx+': '+t)\n",
    "                                q_num2txt[q_idx]=t\n",
    "                                q_n.append(q_idx)\n",
    "                                q_tx.append(t)\n",
    "                                q_pn.append(p_names[i])\n",
    "                                q_type.append('FRQ')\n",
    "                    else:\n",
    "                        q_idx=q_num+'-'+k\n",
    "                        t=qs['context']+qs[k]['original']\n",
    "                        #print(q_idx+': '+t)\n",
    "                        q_num2txt[q_idx]=t\n",
    "                        q_n.append(q_idx)\n",
    "                        q_tx.append(t)\n",
    "                        q_pn.append(p_names[i])\n",
    "                        q_type.append('FRQ')\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            q_idx=q_num\n",
    "            t=qs['original']\n",
    "            #print(q_idx+': '+t)\n",
    "            q_num2txt[q_idx]=t\n",
    "            q_n.append(q_idx)\n",
    "            q_tx.append(t)\n",
    "            q_pn.append(p_names[i])\n",
    "            q_type.append('FRQ')\n",
    "\n",
    "    out_frq_data['Ques_number']=q_n\n",
    "    out_frq_data['Ques_text']=q_tx\n",
    "    out_frq_data['Paper_name']=q_pn\n",
    "    out_frq_data['Ques_type']=q_type\n",
    "\n",
    "\n",
    "    out_data=pd.concat([data_mcq,out_frq_data])\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(out_fp)\n",
    "\n",
    "    # Write each dataframe to a different worksheet.\n",
    "    out_data.to_excel(writer, sheet_name=\"content\",index=False)\n",
    "    head_des=pd.read_excel(r\"D:\\BaiduSyncdisk\\广州外校\\领启\\230512第一阶段\\tool_scripts\\0610split_small_question\\input\\questions_extracted_AL-Chemistry-U1.xlsx\",sheet_name='head_description')\n",
    "    head_des.to_excel(writer, sheet_name=\"head_description\",index=False)\n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.close()\n",
    "    #print(out_data)\n",
    "\n",
    "    \n",
    "def break_q(que,idx_pointers=['({0})'.format(idx) for idx in ['a','b','c','d','e','f','g','h']]):\n",
    "    qs=dict({'original':que})\n",
    "    #print(que)\n",
    "    # extract context\n",
    "    cut_idxs=[que.find(idx) for idx in idx_pointers]\n",
    "    if cut_idxs[0]==-1:\n",
    "        qs['context']='can not break'\n",
    "        return qs\n",
    "    for i in range(len(cut_idxs)-1):\n",
    "        if cut_idxs[i]==-1:\n",
    "            break\n",
    "        if i==0:\n",
    "            qs['context']=que[:cut_idxs[0]]\n",
    "        qs[idx_pointers[i]]=que[cut_idxs[i]:cut_idxs[i+1]]\n",
    "        #print(idx_pointers[i],que[cut_idxs[i]:cut_idxs[i+1]])\n",
    "    return qs\n",
    "\n",
    "\n",
    "def excel_fp2task1_excel(in_fp,debug=False):\n",
    "    fn=in_fp.split('\\\\')[-1]\n",
    "    print(fn)\n",
    "    data=pd.read_excel(in_fp)\n",
    "    if debug:\n",
    "        display(data)\n",
    "        \n",
    "    data_new=pd.DataFrame(columns=['Ques_topic_by_men','Ques_topic_by_GPT',\n",
    "                                   'Similarity','Ques_text','Paper_name',\n",
    "                                   'Ques_number','Ques_type'])\n",
    "    \n",
    "    data_new['Ques_text']=data['agg_txt_full']\n",
    "    data_new['Paper_name']=[fn.replace('.xlsx','')]*len(data)\n",
    "    data_new['Ques_number']=data['agg_qnum']\n",
    "    data_new['Ques_type']=['FRQ']*len(data)\n",
    "    if debug:\n",
    "        display(data_new)\n",
    "    if not os.path.exists(os.path.join(step_dir_7,'task1')):\n",
    "        os.mkdir(os.path.join(step_dir_7,'task1'))\n",
    "\n",
    "    out_ex_fp=os.path.join(step_dir_7,'task1','question_extracted_'+fn)\n",
    "    data_new.to_excel(out_ex_fp,index=False)\n",
    "    fp2fp(out_ex_fp,out_ex_fp)\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    fps=[fp for fp in get_all_files(step_dir_7) if fp.endswith('.xlsx') and 'concate_image' not in fp]\n",
    "    display(fps)\n",
    "    excel_fp2task1_excel(fps[3],debug=True)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concate_image.xlsx\n",
      "wdm11_01_que_20230126.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/2 [00:00<?, ?it/s]Exception in thread Thread-905:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3080, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas\\_libs\\index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas\\_libs\\index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 4554, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 4562, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'agg_txt_full'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\hts\\AppData\\Local\\Temp\\ipykernel_9260\\1041170915.py\", line 10, in compute_and_store_result\n",
      "  File \"C:\\Users\\hts\\AppData\\Local\\Temp\\ipykernel_9260\\3436832812.py\", line 115, in excel_fp2task1_excel\n",
      "  File \"c:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\site-packages\\pandas\\core\\frame.py\", line 3024, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"c:\\Users\\hts\\anaconda3\\envs\\Productivity\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3082, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'agg_txt_full'\n",
      "Progress: 100%|██████████| 2/2 [00:00<00:00, 27.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, 0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps=[fp for fp in get_all_files(step_dir_7) if fp.endswith('.xlsx') and 'concate_image' not in fp]\n",
    "multi(excel_fp2task1_excel,fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
